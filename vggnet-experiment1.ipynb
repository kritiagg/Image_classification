{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import h5py\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    "# Larger CNN for the MNIST Dataset\n",
    "import numpy\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.utils import np_utils\n",
    "# K.set_image_dim_ordering('th')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# variables\n",
    "image_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from scipy.misc import imshow\n",
    "import scipy.misc\n",
    "from numpy import *\n",
    "import PIL\n",
    "\n",
    "\n",
    "def preprocess_image(infilename,size):\n",
    "    data = Image.open(infilename)#.convert('L')\n",
    "#     data = scipy.misc.imread(infilename, mode = \"L\")\n",
    "    data.thumbnail((size,size), Image.ANTIALIAS)\n",
    "    img = data.resize((image_size,image_size))\n",
    "    scipy.misc.imsave(infilename, img)\n",
    "    \n",
    "def load_image( infilename ) :\n",
    "#     data = Image.open(infilename)#.convert('L')\n",
    "    data = scipy.misc.imread(infilename, mode = \"RGB\")\n",
    "#     data.thumbnail((32,32), Image.ANTIALIAS)\n",
    "#     img = data.resize((image_size,image_size))\n",
    "#     preprocess_image(infilename,size)\n",
    "#     data.save(infilename, \"JPEG\")\n",
    "    #img = Image.open(infilename)\n",
    "    #if numberOfColorChannels < 3:\n",
    "      #  print(infilename)\n",
    "#     if img.mode != \"RGB\":\n",
    "#         img = img.convert('RGB')\n",
    "#         img = cv2.cvtColor(img,cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    #img.load()\n",
    "#     scipy.misc.imsave(infilename, img)\n",
    "    #data = np.asarray(img, dtype=\"float32\" )\n",
    "    #img.close()\n",
    "    \n",
    "    return data\n",
    "\n",
    "path = ['Dataset/imgflip_images', 'Dataset/greetings_images','Dataset/scanned_documents','Dataset/imp_images']\n",
    "\n",
    "Y = []\n",
    "X = []\n",
    "data = []\n",
    "\n",
    "for p in path:\n",
    "    for files in listdir(p):\n",
    "        #print(files)\n",
    "        try:\n",
    "            t = load_image(p+'/'+files)\n",
    "            X = t\n",
    "            Y = (float(path.index(p)))\n",
    "            data.append((X,Y))\n",
    "        except:\n",
    "            print(\"error reading file:\"+ files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Xtemp = np.uint8(X)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "random.shuffle(data)\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for d in data:\n",
    "    X.append(d[0])\n",
    "    Y.append(d[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc676b5f630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "#Xtemp = np.uint8(X)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(np.uint8(X[6]))\n",
    "plt.show()\n",
    "print(Y[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "totalSize = len(X)\n",
    "trainingSize = int(0.8*totalSize)\n",
    "# load data\n",
    "X_train = np.array(X[:trainingSize])\n",
    "y_train = np.array(Y[:trainingSize])\n",
    "X_test = np.array(X[trainingSize:])\n",
    "y_test = np.array(Y[trainingSize:])\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# # reshape to be [samples][pixels][width][height]\n",
    "X_train = X_train.reshape(X_train.shape[0], 3, image_size, image_size).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 3, image_size, image_size).astype('float32')\n",
    "# normalize inputs from 0-255 to 0-1\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimension 1 in both shapes must be equal, but are 3 and 64 for 'Assign_136' (op: 'Assign') with input shapes: [3,3,3,64], [3,64,3,3].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    669\u001b[0m           \u001b[0mnode_def_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shapes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensors_as_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m           status)\n\u001b[0m\u001b[1;32m    671\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Dimension 1 in both shapes must be equal, but are 3 and 64 for 'Assign_136' (op: 'Assign') with input shapes: [3,3,3,64], [3,64,3,3].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-b4c683703562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Get back the convolutional part of a VGG network trained on ImageNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_vgg16_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'imagenet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_vgg16_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/applications/vgg16.py\u001b[0m in \u001b[0;36mVGG16\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes)\u001b[0m\n\u001b[1;32m    167\u001b[0m                                     \u001b[0mWEIGHTS_PATH_NO_TOP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                                     cache_subdir='models')\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'theano'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mlayer_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_all_kernels_in_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   2536\u001b[0m             \u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2538\u001b[0;31m             \u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'close'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[0;34m(f, layers)\u001b[0m\n\u001b[1;32m   2968\u001b[0m                              ' elements.')\n\u001b[1;32m   2969\u001b[0m         \u001b[0mweight_value_tuples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2970\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   2146\u001b[0m                 assign_placeholder = tf.placeholder(tf_dtype,\n\u001b[1;32m   2147\u001b[0m                                                     shape=value.shape)\n\u001b[0;32m-> 2148\u001b[0;31m                 \u001b[0massign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_placeholder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_placeholder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assign_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0massign_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking)\u001b[0m\n\u001b[1;32m    549\u001b[0m       \u001b[0mthe\u001b[0m \u001b[0massignment\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mcompleted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \"\"\"\n\u001b[0;32m--> 551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstate_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_locking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_locking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_state_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(ref, value, validate_shape, use_locking, name)\u001b[0m\n\u001b[1;32m     45\u001b[0m   result = _op_def_lib.apply_op(\"Assign\", ref=ref, value=value,\n\u001b[1;32m     46\u001b[0m                                 \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                                 use_locking=use_locking, name=name)\n\u001b[0m\u001b[1;32m     48\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2395\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1755\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/common_shapes.py\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimension 1 in both shapes must be equal, but are 3 and 64 for 'Assign_136' (op: 'Assign') with input shapes: [3,3,3,64], [3,64,3,3]."
     ]
    }
   ],
   "source": [
    "#Get back the convolutional part of a VGG network trained on ImageNet\n",
    "model_vgg16_conv = VGG16(weights='imagenet', include_top=False)\n",
    "model_vgg16_conv.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "#Create your own input format (here 3x200x200)\n",
    "input_size = 32\n",
    "input = Input(shape=(X_train[0].shape),name = 'image_input')\n",
    "print(input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kriti/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"im..., outputs=Tensor(\"pr...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'inbound_nodes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-a8d723f51cb6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Create your own model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m# Compile model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmy_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mnodes_in_progress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1676\u001b[0;31m             \u001b[0mbuild_map_of_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes_in_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes_in_decreasing_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mbuild_map_of_graph\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1664\u001b[0m                 \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m                 build_map_of_graph(x, finished_nodes, nodes_in_progress,\n\u001b[0;32m-> 1666\u001b[0;31m                                    layer, node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mbuild_map_of_graph\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1664\u001b[0m                 \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m                 build_map_of_graph(x, finished_nodes, nodes_in_progress,\n\u001b[0;32m-> 1666\u001b[0;31m                                    layer, node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mbuild_map_of_graph\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1664\u001b[0m                 \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1665\u001b[0m                 build_map_of_graph(x, finished_nodes, nodes_in_progress,\n\u001b[0;32m-> 1666\u001b[0;31m                                    layer, node_index, tensor_index)\n\u001b[0m\u001b[1;32m   1667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1668\u001b[0m             \u001b[0mfinished_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mbuild_map_of_graph\u001b[0;34m(tensor, finished_nodes, nodes_in_progress, layer, node_index, tensor_index)\u001b[0m\n\u001b[1;32m   1662\u001b[0m                 \u001b[0mnode_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m                 \u001b[0mnext_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_nodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m                 build_map_of_graph(x, finished_nodes, nodes_in_progress,\n\u001b[1;32m   1666\u001b[0m                                    layer, node_index, tensor_index)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'inbound_nodes'"
     ]
    }
   ],
   "source": [
    "#Use the generated model \n",
    "output_vgg16_conv = model_vgg16_conv(input)\n",
    "# output_vgg16_conv = K.reshape(output_vgg16_conv,(-1,512,1,1)) \n",
    "\n",
    "#Add the fully-connected layers \n",
    "x = Flatten(name='flatten')(output_vgg16_conv)\n",
    "x = Dense(1024, activation='relu', name='fc1')(x)\n",
    "x = Dense(512, activation='relu', name='fc2')(x)\n",
    "x = Dense(4, activation='softmax', name='predictions')(x)\n",
    "\n",
    "#Create your own model \n",
    "my_model = Model(input=input, output=x,data_format = channels_first)\n",
    "# Compile model\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#In the summary, weights and layers from VGG part will be hidden, but they will be fit during the training\n",
    "my_model.summary()\n",
    "\n",
    "#Then training with your data ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1), Dimension(1), Dimension(512)])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_vgg16_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout = 0.7\n",
    "model = my_model\n",
    "# Fit the model\n",
    "from keras import preprocessing\n",
    "# For preprocessing\n",
    "datagen = ImageDataGenerator(featurewise_center=True,\n",
    "    samplewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    samplewise_std_normalization=True,\n",
    "    zca_whitening=True,\n",
    "    rotation_range=0.,\n",
    "    width_shift_range=0.,\n",
    "    height_shift_range=0.,\n",
    "    shear_range=0.,\n",
    "    zoom_range=0.,\n",
    "    channel_shift_range=0.,\n",
    "    fill_mode='nearest',\n",
    "    cval=0.,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    rescale=None,\n",
    "    preprocessing_function=None,\n",
    "    data_format=K.image_data_format())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32, 3)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected image_input to have shape (None, 32, 32, 3) but got array with shape (3936, 3, 32, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-e0068d026882>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m  \u001b[0;31m#                   steps_per_epoch=len(X_train) / 32, epochs=epochs,callbacks = [history])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Final evaluation of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m                 batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1450\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m             \u001b[0mval_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1303\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1305\u001b[0;31m                                     exception_prefix='input')\n\u001b[0m\u001b[1;32m   1306\u001b[0m         y = _standardize_input_data(y, self._feed_output_names,\n\u001b[1;32m   1307\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/kriti/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    140\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected image_input to have shape (None, 32, 32, 3) but got array with shape (3936, 3, 32, 32)"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "#model.fit_generator(datagen.flow(X_train, y_train, batch_size=32), validation_data=(X_test, y_test),\n",
    " #                   steps_per_epoch=len(X_train) / 32, epochs=epochs,callbacks = [history])\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=200, callbacks = [history])\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Large CNN Error: %.2f%%\" % (100-scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
